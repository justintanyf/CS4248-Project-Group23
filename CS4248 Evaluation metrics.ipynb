{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk\n",
    "%pip install afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, confusion_matrix\n",
    "from afinn import Afinn\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "\n",
    "test = pd.read_excel(\"./raw_data/test.xlsx\").rename({'Satirical =1 Legitimate=0': 'Verdict', 'Full Text ': 'Text'}, axis=1) #raw dataframe\n",
    "y_actual = test['Verdict']\n",
    "y_pred_test = y_pred_test  # add your model's results when predicting on test.xlsx\n",
    "\n",
    "def get_superlatives(text):\n",
    "    superlatives = []\n",
    "    # Tokenize text into words\n",
    "    words = word_tokenize(text)\n",
    "    # Tag words with part-of-speech (POS)\n",
    "    tagged_words = pos_tag(words)\n",
    "    # Filter words tagged as superlatives\n",
    "    for word, pos in tagged_words:\n",
    "        if pos == 'JJS' or word.endswith('est') or word.startswith('most'):\n",
    "            superlatives.append(word)\n",
    "    return superlatives\n",
    "\n",
    "print(\"Test F1 score is:\", f1_score(y_actual, y_pred_test))\n",
    "print(\"Accuracy:\", accuracy_score(y_actual, y_pred_test))\n",
    "print(\"Precision:\", precision_score(y_actual, y_pred_test))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_actual, y_pred_test))\n",
    "\n",
    "# Per y output type\n",
    "# print ('Per y label-----')\n",
    "# for i in range(1, 5):\n",
    "#     y_cat_actual = y_actual_4outputs[y_actual_4outputs == i].apply(lambda x: 1 if x == 4 else 0) \n",
    "#     y_cat_pred = y_pred_test[y_actual_4outputs == i]\n",
    "#     print(\"Accuracy:\", accuracy_score(y_cat_actual, y_cat_pred))\n",
    "#     print(\"Precision:\", precision_score(y_cat_actual, y_cat_pred))\n",
    "#     print(\"Confusion Matrix:\")\n",
    "#     print(confusion_matrix(y_cat_actual, y_cat_pred))\n",
    "\n",
    "print()\n",
    "print('By chracter lengths-----')\n",
    "text_lengths = test['Text'].apply(lambda entry: len(entry.strip()))\n",
    "min_length, max_length = min(text_lengths), max(text_lengths)\n",
    "Q1, Q2, Q3 = np.percentile(text_lengths, 25), np.percentile(text_lengths, 50), np.percentile(text_lengths, 75)\n",
    "text_char_limits = [min_length, Q1, Q2, Q3, max_length]\n",
    "for i in range(4):\n",
    "    y_cat_actual = y_actual[(text_lengths >= text_char_limits[i]) & ((text_lengths <= text_char_limits[i+1]))]\n",
    "    y_cat_pred = y_pred_test[(text_lengths >= text_char_limits[i]) & ((text_lengths <= text_char_limits[i+1]))]\n",
    "    print(f'F1 score for {i+1}th quartile: {f1_score(y_cat_actual, y_cat_pred)}')\n",
    "    print(f'Accuracy for {i+1}th quartile: {accuracy_score(y_cat_actual, y_cat_pred)}')\n",
    "    print(f'Precision for {i+1}th quartile: {precision_score(y_cat_actual, y_cat_pred)}')\n",
    "\n",
    "\n",
    "print()\n",
    "print('By domain')\n",
    "domains = test['Domain'].unique()\n",
    "for domain in domains:\n",
    "    y_cat_actual = y_actual[test['Domain'] == domain]\n",
    "    y_cat_pred = y_pred_test[test['Domain'] == domain]\n",
    "    print(f'Entries in {domain}: {len(y_cat_actual)}')\n",
    "    print(f'F1 score for {domain}: {f1_score(y_cat_actual, y_cat_pred)}')\n",
    "    print(f'Accuracy for {domain}: {accuracy_score(y_cat_actual, y_cat_pred)}')\n",
    "    print(f'Precision for {domain}: {precision_score(y_cat_actual, y_cat_pred)}')\n",
    "\n",
    "print()\n",
    "print('By Subtopic')\n",
    "subtopics = test['Subtopic'].unique()\n",
    "for subtopic in subtopics:\n",
    "    y_cat_actual = y_actual[test['Subtopic'] == subtopic]\n",
    "    y_cat_pred = y_pred_test[test['Subtopic'] == subtopic]\n",
    "    print(f'Entries in {subtopic}: {len(y_cat_actual)}')\n",
    "    print(f'F1 score for {subtopic}: {f1_score(y_cat_actual, y_cat_pred)}')\n",
    "    print(f'Accuracy for {subtopic}: {accuracy_score(y_cat_actual, y_cat_pred)}')\n",
    "    print(f'Precision for {subtopic}: {precision_score(y_cat_actual, y_cat_pred)}')\n",
    "\n",
    "print()\n",
    "print('By superlatives per sentence')\n",
    "superlatives_per_length = test['Text'].apply(lambda entry: len(get_superlatives(entry))/len(entry.strip()))\n",
    "print(superlatives_per_length)\n",
    "min_length, max_length = min(superlatives_per_length), max(superlatives_per_length)\n",
    "Q1, Q2, Q3 = np.percentile(superlatives_per_length, 25), np.percentile(superlatives_per_length, 50), np.percentile(superlatives_per_length, 75)\n",
    "superlative_limits = [min_length, Q1, Q2, Q3, max_length]\n",
    "for i in range(4):\n",
    "    y_cat_actual = y_actual[(superlatives_per_length >= superlative_limits[i]) & ((superlatives_per_length <= superlative_limits[i+1]))]\n",
    "    y_cat_pred = y_pred_test[(superlatives_per_length >= superlative_limits[i]) & ((superlatives_per_length <= superlative_limits[i+1]))]\n",
    "    print(f'F1 score for {i+1}th quartile: {f1_score(y_cat_actual, y_cat_pred)}')\n",
    "    print(f'Accuracy for {i+1}th quartile: {accuracy_score(y_cat_actual, y_cat_pred)}')\n",
    "    print(f'Precision for {i+1}th quartile: {precision_score(y_cat_actual, y_cat_pred)}')\n",
    "\n",
    "print()\n",
    "print('By sentiment analysis (+ve = happy, -ve = sad)')\n",
    "afn = Afinn()\n",
    "sentiment_scores = test['Text'].apply(afn.score).apply(lambda s: 1 if s > 3 else -1 if s < -3 else 0)\n",
    "all_scores = sentiment_scores.unique()\n",
    "for score in all_scores:\n",
    "    y_cat_actual = y_actual[sentiment_scores == score]\n",
    "    y_cat_pred = y_pred_test[sentiment_scores == score]\n",
    "    print(f'Entries for score {score}: {len(y_cat_actual)}')\n",
    "    print(f'F1 score for score {score}: {f1_score(y_cat_actual, y_cat_pred)}')\n",
    "    print(f'Accuracy for score {score}: {accuracy_score(y_cat_actual, y_cat_pred)}')\n",
    "    print(f'Precision for score {score}: {precision_score(y_cat_actual, y_cat_pred)}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
