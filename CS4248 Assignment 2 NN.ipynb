{"cells":[{"cell_type":"markdown","metadata":{"id":"oyI5SA6HIoRH"},"source":["# CS4248 Project Group 23"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"vS033yHSHyAx"},"outputs":[],"source":["# If you wish to run this on Google Colab, mount the Google drive by running this cell or click the `files` icon on the left navbar\n","# and click mount Google Drive (it takes some time to load)\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","# %cd \"/content/drive/My Drive/<The path to this notebook in your Google Drive>\"\n","# !cd \"/content/drive/My Drive/<The path to this notebook in your Google Drive>\""]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["MPS device not found.\n"]}],"source":["# To check if torch is working on m1\n","import torch\n","if torch.backends.mps.is_available():\n","    mps_device = torch.device(\"mps\")\n","    x = torch.ones(1, device=mps_device)\n","    print (x)\n","else:\n","    print (\"MPS device not found.\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"2nqSeqYQJ4vW"},"outputs":[],"source":["import pandas as pd\n","from sklearn.metrics import f1_score\n","import numpy as np"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Unzip raw_data.zip locally\n","import zipfile\n","with zipfile.ZipFile('raw_data.zip', 'r') as zip_ref:\n","    zip_ref.extractall()"]},{"cell_type":"markdown","metadata":{},"source":["Feature Engineering: Capture various features of the text (e.g. punctuation, stopwords, statement length). \n","Test out different tokenizers to capture their performance.\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Done. 400000  words loaded!\n"]}],"source":["import gensim.downloader as api\n","\n","def load_glove_model():\n","    glove_model = api.load('glove-wiki-gigaword-300')\n","    print(\"Done.\",len(glove_model),\" words loaded!\")\n","    return glove_model, glove_model.vector_size\n","\n","glove_model, glove_dim = load_glove_model()"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import re\n","import string\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","# todo parallelize this in future\n","\n","\n","def preprocess_text(text):\n","    # Lowercase text\n","    text = text.lower()\n","    # Remove punctuation\n","    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n","    # tokenize\n","    words = text.split()\n","    # Get the GloVe vectors\n","    vectors = [glove_model[word] for word in words if word in glove_model]\n","    # If vectors is empty, return a vector of zeros\n","    if not vectors:\n","        print(\"No vectors found for the text: \", text)\n","        return np.zeros(glove_model.vector_size)\n","    return np.mean(vectors, axis=0)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"9PepPt_CL94x"},"outputs":[{"name":"stdout","output_type":"stream","text":["No vectors found for the text:  abfs daksl \n","No vectors found for the text:  nsas xkeyscore explainedyoutube \n"]}],"source":["train = pd.read_csv(\"./raw_data/fulltrain.csv\", names=['Verdict', 'Text'])\n","x_train = np.array([preprocess_text(text) for text in train['Text']]) \n","y_train = train['Verdict'].values - 1 # subtract 1 to make the labels 0-based"]},{"cell_type":"markdown","metadata":{},"source":["Test out different kinds of models and find the most effective architectures.|"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","class SimpleNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(SimpleNN, self).__init__()\n","        self.fc1 = nn.Linear(input_size, hidden_size) \n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_size, num_classes)  \n","    \n","    def forward(self, x):\n","        out = self.fc1(x)\n","        out = self.relu(out)\n","        out = self.fc2(out)\n","        return out"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"ename":"ImportError","evalue":"cannot import name 'tfidfVectorizer' from 'sklearn.feature_extraction.text' (/home/t1dus/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Vectorize the text if you didn't use GloVe\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tfidfVectorizer\n\u001b[1;32m      3\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m tfidfVectorizer()\n\u001b[1;32m      4\u001b[0m vectorizer\u001b[38;5;241m.\u001b[39mfit(x_train)\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'tfidfVectorizer' from 'sklearn.feature_extraction.text' (/home/t1dus/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py)"]}],"source":["# Vectorize the text if you didn't use GloVe\n","from sklearn.feature_extraction.text import tfidfVectorizer\n","vectorizer = tfidfVectorizer()\n","vectorizer.fit(x_train)\n","x_train = vectorizer.transform(x_train).toarray()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_model(model, X_train, y_train, num_epochs=5, learning_rate=0.001):\n","    # Convert numpy arrays to PyTorch tensors\n","    X_train = torch.from_numpy(X_train).float()\n","    y_train = torch.from_numpy(y_train).long()\n","\n","    # Create a DataLoader for the training data\n","    train_data = TensorDataset(X_train, y_train)\n","    train_loader = DataLoader(train_data, batch_size=16)\n","\n","    # Define the loss function and optimizer\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Train the model\n","    for epoch in range(num_epochs):\n","        for i, (texts, labels) in enumerate(train_loader):\n","            outputs = model(texts)\n","            loss = criterion(outputs, labels)\n","            \n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# train your model\n","model = SimpleNN(input_size=300, hidden_size=100, num_classes=4) # for now assume that we are doing the 4 class labels (0, 1, 2, 3\n","train_model(model, x_train, y_train)\n","\n","x_train_tensor = torch.from_numpy(x_train).float()\n","y_pred = model(x_train_tensor).argmax(dim=1).numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["# Or load a previously saved model\n","model = SimpleNN(input_size=300, hidden_size=100, num_classes=4)\n","model.load_state_dict(torch.load('./trained_models/model.pth'))"]},{"cell_type":"markdown","metadata":{},"source":["Perform hyperparameter tuning on best 3 models."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","\n","# or perform hyperparameter tuning\n","# Create the hyperparameters grid\n","param_grid = {\n","\n","}\n","\n","grid_search = GridSearchCV(())\n","\n","# Train\n","grid_search.fit(x_train, y_train)\n","\n","print(grid_search.best_params_)\n","\n","# Use best model\n","model = grid_search.best_estimator_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Th6eT-_1Lu_e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Set: 0.8681960038153329\n"]}],"source":["# get the training error\n","f1_score(y_train, y_pred, average='macro')\n","# print for the train set f1 score is\n","print(\"Train Set: \" + str(f1_score(y_train, y_pred, average='macro')))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMLL1rU_n4BP"},"outputs":[{"ename":"TypeError","evalue":"'LogisticRegression' object is not callable","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([preprocess_text(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m test\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m]]) \n\u001b[1;32m      4\u001b[0m X_test_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(X_test)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m----> 5\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m _, result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# adjust the labels in the test set to be in the range 0-3\u001b[39;00m\n","\u001b[0;31mTypeError\u001b[0m: 'LogisticRegression' object is not callable"]}],"source":["# get the prediction for the test set\n","test = pd.read_csv('./raw_data/balancedtest.csv')\n","X_test = np.array([preprocess_text(text) for text in test.iloc[:, 1]]) \n","X_test_tensor = torch.from_numpy(X_test).float()\n","output = model(X_test_tensor)\n","_, result = torch.max(output.data, 1)\n","\n","# adjust the labels in the test set to be in the range 0-3\n","y_test = test.iloc[:, 0].values - 1\n","\n","# get the f1 score against the test set\n","print(\"Test Set: \" + str(f1_score(y_test, result.numpy(), average='macro')))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save the model\n","torch.save(model.state_dict(), './trained_models/model.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO5ywtC5STPqHlkhNNBbfoT","name":"CS4248 Assignment 2.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
