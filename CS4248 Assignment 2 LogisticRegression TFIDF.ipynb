{"cells":[{"cell_type":"markdown","metadata":{"id":"oyI5SA6HIoRH"},"source":["# CS4248 Project Group 23"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"vS033yHSHyAx"},"outputs":[],"source":["# If you wish to run this on Google Colab, mount the Google drive by running this cell or click the `files` icon on the left navbar\n","# and click mount Google Drive (it takes some time to load)\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","# %cd \"/content/drive/My Drive/<The path to this notebook in your Google Drive>\"\n","# !cd \"/content/drive/My Drive/<The path to this notebook in your Google Drive>\""]},{"cell_type":"code","execution_count":2,"metadata":{"id":"2nqSeqYQJ4vW"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.metrics import f1_score\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Unzip raw_data.zip locally\n","import zipfile\n","with zipfile.ZipFile('raw_data.zip', 'r') as zip_ref:\n","    zip_ref.extractall()"]},{"cell_type":"markdown","metadata":{},"source":["Feature Engineering: Capture various features of the text (e.g. punctuation, stopwords, statement length). \n","Test out different tokenizers to capture their performance.\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import re\n","import string\n","from nltk.tokenize import word_tokenize\n","\n","# todo parallelize this in future\n","\n","\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Initialize the TF-IDF vectorizer\n","vectorizer = TfidfVectorizer()\n","\n","def preprocess_text(text):\n","    # Lowercase text\n","    text = text.lower()\n","    # Remove punctuation\n","    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n","    # tokenize\n","    words = word_tokenize(text)\n","    # Initialize lemmatizer\n","    lemmatizer = WordNetLemmatizer()\n","    # Apply lemmatization\n","    words = [lemmatizer.lemmatize(word) for word in words]\n","    # Convert list of words back to string\n","    text = ' '.join(words)\n","    return text"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package omw-1.4 to /home/t1dus/nltk_data...\n"]},{"data":{"text/plain":["True"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('omw-1.4')"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"9PepPt_CL94x"},"outputs":[],"source":["train = pd.read_csv(\"./raw_data/fulltrain.csv\", names=['Verdict', 'Text'])\n","test = pd.read_csv(\"./raw_data/balancedtest.csv\", names=['Verdict', 'Text'])\n","\n","# Preprocess all documents in the corpus\n","preprocessed_corpus = [preprocess_text(text) for text in train['Text']]\n","\n","# Fit the TF-IDF vectorizer on the preprocessed corpus\n","X_train = vectorizer.fit_transform(preprocessed_corpus)\n","X_test = vectorizer.transform([preprocess_text(text) for text in test['Text']])\n","\n","y_train = train['Verdict'].apply(lambda x: 1 if x == 4 else 0) # convert to binary- label 4 = trusted\n","y_test = test['Verdict'].apply(lambda x: 1 if x == 4 else 0) # convert to binary- label 4 = trusted"]},{"cell_type":"markdown","metadata":{},"source":["Test out different kinds of models and find the most effective architectures.|"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","model = LogisticRegression(max_iter=10000)\n","model.fit(X_train, y_train)\n","y_pred_train = model.predict(X_train)\n","y_pred_test = model.predict(X_test)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training F1 score is: 0.9318290163673925\n","Test F1 score is: 0.8406827880512091\n"]}],"source":["print(\"Training F1 score is:\", f1_score(y_train, y_pred_train))\n","print(\"Test F1 score is:\", f1_score(y_test, y_pred_test))"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["['./sklearn_models/logisticRegression 0.8406827880512091tfidf.pkl']"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# save new model\n","import joblib\n","joblib.dump(model, './sklearn_models/logisticRegression ' + str(f1_score(y_test, y_pred_test)) + 'tfidf.pkl')"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: './sklearn_models/logisticRegression.pkl'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load a previous model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./sklearn_models/logisticRegression.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train)\n","File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './sklearn_models/logisticRegression.pkl'"]}],"source":["# load a previous model\n","import joblib\n","model = joblib.load(\"./sklearn_models/logisticRegression.pkl\")\n","y_pred = model.predict(X_train)"]},{"cell_type":"markdown","metadata":{},"source":["Perform hyperparameter tuning on best 3 models."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","\n","# or perform hyperparameter tuning\n","# Create the hyperparameters grid\n","param_grid = {\n","\n","}\n","\n","grid_search = GridSearchCV(())\n","\n","# Train\n","grid_search.fit(X_train, y_train)\n","\n","print(grid_search.best_params_)\n","\n","# Use best model\n","model = grid_search.best_estimator_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO5ywtC5STPqHlkhNNBbfoT","name":"CS4248 Assignment 2.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
