{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyI5SA6HIoRH"
   },
   "source": [
    "# CS4248 Project Group 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vS033yHSHyAx"
   },
   "outputs": [],
   "source": [
    "# If you wish to run this on Google Colab, mount the Google drive by running this cell or click the `files` icon on the left navbar\n",
    "# and click mount Google Drive (it takes some time to load)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# %cd \"/content/drive/My Drive/<The path to this notebook in your Google Drive>\"\n",
    "# !cd \"/content/drive/My Drive/<The path to this notebook in your Google Drive>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check if torch is working on m1\n",
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nqSeqYQJ4vW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip raw_data.zip locally\n",
    "import zipfile\n",
    "with zipfile.ZipFile('raw_data.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering: Capture various features of the text (e.g. punctuation, stopwords, statement length). \n",
    "Test out different tokenizers to capture their performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "def load_glove_model():\n",
    "    glove_model = api.load('glove-wiki-gigaword-300')\n",
    "    print(\"Done.\",len(glove_model),\" words loaded!\")\n",
    "    return glove_model, glove_model.vector_size\n",
    "\n",
    "glove_model, glove_dim = load_glove_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# todo parallelize this in future\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase text\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    # tokenize\n",
    "    words = text.split()\n",
    "    # Get the GloVe vectors\n",
    "    vectors = [glove_model[word] for word in words if word in glove_model]\n",
    "    # If vectors is empty, return a vector of zeros\n",
    "    if not vectors:\n",
    "        print(\"No vectors found for the text: \", text)\n",
    "        return np.zeros(glove_model.vector_size)\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9PepPt_CL94x"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./raw_data/fulltrain.csv\", names=['Verdict', 'Text'])\n",
    "x_train = np.array([preprocess_text(text) for text in train['Text']]) \n",
    "# y_train = train['Verdict'].values # - 1 # subtract 1 to make the labels 0-based\n",
    "y_train = train['Verdict'].apply(lambda x: 1 if x == 4 else 0).values # convert to binary- label 4 = trusted\n",
    "print(y_train)\n",
    "y_train = y_train.reshape((-1, 1))\n",
    "\n",
    "test = pd.read_csv(\"./raw_data/balancedtest.csv\", names=['Verdict', 'Text'])\n",
    "X_test = np.array([preprocess_text(text) for text in test['Text']]) \n",
    "y_test = test['Verdict'].apply(lambda x: 1 if x == 4 else 0).values # convert to binary- label 4 = trusted\n",
    "y_test = y_test.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out different kinds of models and find the most effective architectures.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 300) \n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(300, 200)  \n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(200, 100)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(100, num_classes)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.fc4(out)\n",
    "        return self.sigmoid(out) #self.softmax(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text if you didn't use GloVe\n",
    "from sklearn.feature_extraction.text import tfidfVectorizer\n",
    "vectorizer = tfidfVectorizer()\n",
    "vectorizer.fit(x_train)\n",
    "x_train = vectorizer.transform(x_train).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_model(model, X_train, y_train, num_epochs=5, learning_rate=0.001):\n",
    "    global epochs\n",
    "    global loss_arr\n",
    "    global f1_score_arr\n",
    "    epochs = []\n",
    "    loss_arr = []\n",
    "    f1_score_arr = []\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_train = torch.from_numpy(X_train).float()\n",
    "    y_train = torch.from_numpy(y_train).float()\n",
    "    print(X_train)\n",
    "    print(y_train)\n",
    "    # Create a DataLoader for the training data\n",
    "    train_data = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_data, batch_size=y_train.shape[0])\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    best_f1_score = 0.0\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        epochs.append(epoch + 1)\n",
    "        epoch_loss = 0\n",
    "        for i, (texts, labels) in enumerate(train_loader):\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        print (\"Epoch: {}, Loss: {}\".format(epoch, epoch_loss))\n",
    "        output = model(X_test_tensor)\n",
    "        result = (output.data > 0.5).long()\n",
    "        test_f1_score = f1_score(y_test, result.numpy(), average='macro')\n",
    "        if (test_f1_score > best_f1_score):\n",
    "            best_f1_score = test_f1_score\n",
    "            print(\"new best:\")\n",
    "        print(test_f1_score)\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "        loss_arr.append(epoch_loss)\n",
    "        f1_score_arr.append(test_f1_score)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train your model\n",
    "\n",
    "print(x_train)\n",
    "print(y_train)\n",
    "\n",
    "model = SimpleNN(input_size=300, num_classes=1) \n",
    "train_model(model, x_train, y_train, num_epochs=400, learning_rate=0.0005)\n",
    "\n",
    "x_train_tensor = torch.from_numpy(x_train).float()\n",
    "print(model(x_train_tensor))\n",
    "y_pred = model(x_train_tensor).round().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import seaborn\n",
    "# pyplot.plot(epochs, loss_arr, epochs, f1_score_arr)\n",
    "# pyplot.title(\"Plot of F1-score and Training loss vs # of epochs\")\n",
    "seaborn.plotting_context(\"poster\")\n",
    "\n",
    "seaborn.lineplot(x=epochs, y=loss_arr)\n",
    "seaborn.lineplot(x=epochs, y=f1_score_arr)\n",
    "pyplot.title(\"Plot of F1-score and Training loss vs number of epochs\")\n",
    "pyplot.xlabel(\"Number of Epochs\")\n",
    "pyplot.ylabel(\"Training Loss (blue) or F1-Score (orange)\")\n",
    "seaborn.plotting_context(\"poster\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or load a previously saved model\n",
    "#model = SimpleNN(input_size=300, hidden_size=100, num_classes=4)\n",
    "model.load_state_dict(torch.load('./trained_models/model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform hyperparameter tuning on best 3 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# or perform hyperparameter tuning\n",
    "# Create the hyperparameters grid\n",
    "param_grid = {\n",
    "\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(())\n",
    "\n",
    "# Train\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Use best model\n",
    "model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Th6eT-_1Lu_e"
   },
   "outputs": [],
   "source": [
    "# get the training error\n",
    "print(y_train)\n",
    "print(y_pred)\n",
    "f1_score(y_train, y_pred, average='macro')\n",
    "# print for the train set f1 score is\n",
    "print(\"Train Set: \" + str(f1_score(y_train, y_pred, average='macro')))\n",
    "print(\"Train Set Accuracy: \" + str(np.count_nonzero(y_train == y_pred) / y_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMLL1rU_n4BP"
   },
   "outputs": [],
   "source": [
    "# get the prediction for the test set\n",
    "test = pd.read_csv('./raw_data/balancedtest.csv')\n",
    "X_test = np.array([preprocess_text(text) for text in test.iloc[:, 1]]) \n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "output = model(X_test_tensor)\n",
    "result = (output.data > 0.5).long()\n",
    "# _, result = torch.max(output.data, 1)\n",
    "\n",
    "# adjust the labels in the test set to be in the range 0-3\n",
    "#y_test = test.iloc[:, 0].values \n",
    "y_test = test.iloc[:, 0].apply(lambda x: 1 if x == 4 else 0).values # convert to binary- label 4 = trusted\n",
    "y_test = y_test.reshape((-1, 1))\n",
    "\n",
    "#print(y_test)\n",
    "#print(result.numpy())\n",
    "\n",
    "#for i in range(y_test.shape[0]):\n",
    "#    print(y_test[i])\n",
    "#    print(result.numpy()[i])\n",
    "\n",
    "# get the f1 score against the test set\n",
    "print(\"Test Set F1: \" + str(f1_score(y_test, result.numpy(), average='macro')))\n",
    "print(result.numpy())\n",
    "print(y_test)\n",
    "print(np.count_nonzero(y_test == result.numpy()))\n",
    "print(y_test.shape)\n",
    "print(\"Test Set Accuracy: \" + str(np.count_nonzero(y_test == result.numpy()) / y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), './trained_models/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, confusion_matrix\n",
    "from afinn import Afinn\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "\n",
    "test = pd.read_excel(\"./raw_data/test.xlsx\").rename({'Satirical =1 Legitimate=0': 'Verdict', 'Full Text ': 'Text'}, axis=1) #raw dataframe\n",
    "#print(test.iloc[:, 2][0])\n",
    "X_test2 = np.array([preprocess_text(text) for text in test.iloc[:, 2]])\n",
    "X_test_tensor2 = torch.from_numpy(X_test2).float()\n",
    "output2 = model(X_test_tensor2)\n",
    "print(X_test2)\n",
    "print(output2)\n",
    "result2 = 1 - (output2.data > 0.5).long().detach().numpy()#(output2.data > 0.8).long() # My model trains on Satirical = 0, Legitimate = 1\n",
    "y_actual = test['Verdict']\n",
    "y_pred_test = result2  # add your model's results when predicting on test.xlsx\n",
    "\n",
    "print(y_pred_test.shape)\n",
    "\n",
    "#for i in range(360):\n",
    "#    print(y_actual[i])\n",
    "#    print(y_pred_test[i])\n",
    "\n",
    "def get_superlatives(text):\n",
    "    superlatives = []\n",
    "    # Tokenize text into words\n",
    "    words = word_tokenize(text)\n",
    "    # Tag words with part-of-speech (POS)\n",
    "    tagged_words = pos_tag(words)\n",
    "    # Filter words tagged as superlatives\n",
    "    for word, pos in tagged_words:\n",
    "        if pos == 'JJS' or word.endswith('est') or word.startswith('most'):\n",
    "            superlatives.append(word)\n",
    "    return superlatives\n",
    "\n",
    "print(\"Test F1 score is:\", f1_score(y_actual, y_pred_test))\n",
    "print(\"Accuracy:\", accuracy_score(y_actual, y_pred_test))\n",
    "print(\"Precision:\", precision_score(y_actual, y_pred_test))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_actual, y_pred_test))\n",
    "\n",
    "# Per y output type\n",
    "# print ('Per y label-----')\n",
    "# for i in range(1, 5):\n",
    "#     y_cat_actual = y_actual_4outputs[y_actual_4outputs == i].apply(lambda x: 1 if x == 4 else 0) \n",
    "#     y_cat_pred = y_pred_test[y_actual_4outputs == i]\n",
    "#     print(\"Accuracy:\", accuracy_score(y_cat_actual, y_cat_pred))\n",
    "#     print(\"Precision:\", precision_score(y_cat_actual, y_cat_pred))\n",
    "#     print(\"Confusion Matrix:\")\n",
    "#     print(confusion_matrix(y_cat_actual, y_cat_pred))\n",
    "\n",
    "print()\n",
    "print('By chracter lengths-----')\n",
    "text_lengths = test['Text'].apply(lambda entry: len(entry.strip()))\n",
    "min_length, max_length = min(text_lengths), max(text_lengths)\n",
    "Q1, Q2, Q3 = np.percentile(text_lengths, 25), np.percentile(text_lengths, 50), np.percentile(text_lengths, 75)\n",
    "text_char_limits = [min_length, Q1, Q2, Q3, max_length]\n",
    "for i in range(4):\n",
    "    y_cat_actual = y_actual[(text_lengths >= text_char_limits[i]) & ((text_lengths <= text_char_limits[i+1]))]\n",
    "    y_cat_pred = y_pred_test[(text_lengths >= text_char_limits[i]) & ((text_lengths <= text_char_limits[i+1]))]\n",
    "    print(f'F1 score for {i+1}th quartile: {f1_score(y_cat_actual, y_cat_pred)}')\n",
    "    print(f'Accuracy for {i+1}th quartile: {accuracy_score(y_cat_actual, y_cat_pred)}')\n",
    "    print(f'Precision for {i+1}th quartile: {precision_score(y_cat_actual, y_cat_pred)}')\n",
    "\n",
    "\n",
    "print()\n",
    "print('By domain')\n",
    "domains = test['Domain'].unique()\n",
    "for domain in domains:\n",
    "    y_cat_actual = y_actual[test['Domain'] == domain]\n",
    "    y_cat_pred = y_pred_test[test['Domain'] == domain]\n",
    "    print(f'Entries in {domain}: {len(y_cat_actual)}')\n",
    "    print(f'F1 score for {domain}: {f1_score(y_cat_actual, y_cat_pred)}')\n",
    "    print(f'Accuracy for {domain}: {accuracy_score(y_cat_actual, y_cat_pred)}')\n",
    "    print(f'Precision for {domain}: {precision_score(y_cat_actual, y_cat_pred)}')\n",
    "\n",
    "print()\n",
    "print('By Subtopic')\n",
    "subtopics = test['Subtopic'].unique()\n",
    "for subtopic in subtopics:\n",
    "    y_cat_actual = y_actual[test['Subtopic'] == subtopic]\n",
    "    y_cat_pred = y_pred_test[test['Subtopic'] == subtopic]\n",
    "    print(f'Entries in {subtopic}: {len(y_cat_actual)}')\n",
    "    print(f'F1 score for {subtopic}: {f1_score(y_cat_actual, y_cat_pred)}')\n",
    "    print(f'Accuracy for {subtopic}: {accuracy_score(y_cat_actual, y_cat_pred)}')\n",
    "    print(f'Precision for {subtopic}: {precision_score(y_cat_actual, y_cat_pred)}')\n",
    "\n",
    "print()\n",
    "print('By superlatives per sentence')\n",
    "superlatives_per_length = test['Text'].apply(lambda entry: len(get_superlatives(entry))/len(entry.strip()))\n",
    "print(superlatives_per_length)\n",
    "min_length, max_length = min(superlatives_per_length), max(superlatives_per_length)\n",
    "Q1, Q2, Q3 = np.percentile(superlatives_per_length, 25), np.percentile(superlatives_per_length, 50), np.percentile(superlatives_per_length, 75)\n",
    "superlative_limits = [min_length, Q1, Q2, Q3, max_length]\n",
    "for i in range(4):\n",
    "    y_cat_actual = y_actual[(superlatives_per_length >= superlative_limits[i]) & ((superlatives_per_length <= superlative_limits[i+1]))]\n",
    "    y_cat_pred = y_pred_test[(superlatives_per_length >= superlative_limits[i]) & ((superlatives_per_length <= superlative_limits[i+1]))]\n",
    "    print(f'F1 score for {i+1}th quartile: {f1_score(y_cat_actual, y_cat_pred)}')\n",
    "    print(f'Accuracy for {i+1}th quartile: {accuracy_score(y_cat_actual, y_cat_pred)}')\n",
    "    print(f'Precision for {i+1}th quartile: {precision_score(y_cat_actual, y_cat_pred)}')\n",
    "\n",
    "print()\n",
    "print('By sentiment analysis (+ve = happy, -ve = sad)')\n",
    "afn = Afinn()\n",
    "sentiment_scores = test['Text'].apply(afn.score).apply(lambda s: 1 if s > 3 else -1 if s < -3 else 0)\n",
    "all_scores = sentiment_scores.unique()\n",
    "for score in all_scores:\n",
    "    y_cat_actual = y_actual[sentiment_scores == score]\n",
    "    y_cat_pred = y_pred_test[sentiment_scores == score]\n",
    "    print(f'Entries for score {score}: {len(y_cat_actual)}')\n",
    "    print(f'F1 score for score {score}: {f1_score(y_cat_actual, y_cat_pred)}')\n",
    "    print(f'Accuracy for score {score}: {accuracy_score(y_cat_actual, y_cat_pred)}')\n",
    "    print(f'Precision for score {score}: {precision_score(y_cat_actual, y_cat_pred)}')\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO5ywtC5STPqHlkhNNBbfoT",
   "name": "CS4248 Assignment 2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
