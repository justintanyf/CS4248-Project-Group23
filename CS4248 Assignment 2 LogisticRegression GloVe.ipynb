{"cells":[{"cell_type":"markdown","metadata":{"id":"oyI5SA6HIoRH"},"source":["# CS4248 Project Group 23"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vS033yHSHyAx"},"outputs":[],"source":["# If you wish to run this on Google Colab, mount the Google drive by running this cell or click the `files` icon on the left navbar\n","# and click mount Google Drive (it takes some time to load)\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd \"/content/drive/My Drive/<The path to this notebook in your Google Drive>\"\n","!cd \"/content/drive/My Drive/<The path to this notebook in your Google Drive>\""]},{"cell_type":"code","execution_count":1,"metadata":{"id":"2nqSeqYQJ4vW"},"outputs":[],"source":["import pandas as pd\n","from sklearn.metrics import f1_score\n","import numpy as np"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Unzip raw_data.zip locally\n","import zipfile\n","with zipfile.ZipFile('raw_data.zip', 'r') as zip_ref:\n","    zip_ref.extractall()"]},{"cell_type":"markdown","metadata":{},"source":["Feature Engineering: Capture various features of the text (e.g. punctuation, stopwords, statement length). \n","Test out different tokenizers to capture their performance.\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Done. 400000  words loaded!\n"]}],"source":["import gensim.downloader as api\n","\n","def load_glove_model():\n","    glove_model = api.load('glove-wiki-gigaword-300')\n","    print(\"Done.\",len(glove_model),\" words loaded!\")\n","    return glove_model, glove_model.vector_size\n","\n","glove_model, glove_dim = load_glove_model()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import re\n","import string\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","\n","# todo parallelize this in future\n","\n","\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","\n","def preprocess_text(text):\n","    # Lowercase text\n","    text = text.lower()\n","    # Remove punctuation\n","    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n","    # tokenize\n","    words = word_tokenize(text)\n","    # Initialize lemmatizer\n","    lemmatizer = WordNetLemmatizer()\n","    # Apply lemmatization\n","    words = [lemmatizer.lemmatize(word) for word in words]\n","    # Get the GloVe vectors\n","    vectors = [glove_model[word] for word in words if word in glove_model]\n","    # If vectors is empty, return a vector of zeros\n","    if not vectors:\n","        print(\"No vectors found for the text: \", text)\n","        return np.zeros(glove_model.vector_size)\n","    return np.mean(vectors, axis=0)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"9PepPt_CL94x"},"outputs":[{"name":"stdout","output_type":"stream","text":["No vectors found for the text:  abfs daksl \n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./raw_data/fulltrain.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m x_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([preprocess_text(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m train\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m]]) \n\u001b[1;32m      3\u001b[0m y_train \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# subtract 1 to make the labels 0-based\u001b[39;00m\n","Cell \u001b[0;32mIn[4], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./raw_data/fulltrain.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m x_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m train\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m]]) \n\u001b[1;32m      3\u001b[0m y_train \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# subtract 1 to make the labels 0-based\u001b[39;00m\n","Cell \u001b[0;32mIn[3], line 24\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     22\u001b[0m words \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Get the GloVe vectors\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m vectors \u001b[38;5;241m=\u001b[39m [glove_model[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m glove_model]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# If vectors is empty, return a vector of zeros\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vectors:\n","Cell \u001b[0;32mIn[3], line 24\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m words \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Get the GloVe vectors\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m vectors \u001b[38;5;241m=\u001b[39m [glove_model[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mglove_model\u001b[49m]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# If vectors is empty, return a vector of zeros\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vectors:\n","File \u001b[0;32m~/opt/miniconda3/envs/CS4248NLP/lib/python3.10/site-packages/gensim/models/keyedvectors.py:649\u001b[0m, in \u001b[0;36mKeyedVectors.__contains__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__contains__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_index_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/opt/miniconda3/envs/CS4248NLP/lib/python3.10/site-packages/gensim/models/keyedvectors.py:646\u001b[0m, in \u001b[0;36mKeyedVectors.has_index_for\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhas_index_for\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m    638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can this model return a single index for this key?\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m    Subclasses that synthesize vectors for out-of-vocabulary words (like\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    644\u001b[0m \n\u001b[1;32m    645\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 646\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n","File \u001b[0;32m~/opt/miniconda3/envs/CS4248NLP/lib/python3.10/site-packages/gensim/models/keyedvectors.py:412\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_index\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    408\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the integer index (slot/position) where the given key's vector is stored in the\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;124;03m    backing vectors array.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \n\u001b[1;32m    411\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 412\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_to_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m val\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["train = pd.read_csv(\"./raw_data/fulltrain.csv\")\n","x_train = np.array([preprocess_text(text) for text in train.iloc[:, 1]]) \n","y_train = train.iloc[:, 0].values - 1 # subtract 1 to make the labels 0-based"]},{"cell_type":"markdown","metadata":{},"source":["Test out different kinds of models and find the most effective architectures.|"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","model = LogisticRegression(max_iter=10000)\n","model.fit(x_train, y_train)\n","y_pred = model.predict(x_train)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# load a previous model\n","import joblib\n","model = joblib.load(\"./sklearn_models/logisticRegression.pkl\")\n","y_pred = model.predict(x_train)"]},{"cell_type":"markdown","metadata":{},"source":["Perform hyperparameter tuning on best 3 models."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","\n","# or perform hyperparameter tuning\n","# Create the hyperparameters grid\n","param_grid = {\n","\n","}\n","\n","grid_search = GridSearchCV(())\n","\n","# Train\n","grid_search.fit(x_train, y_train)\n","\n","print(grid_search.best_params_)\n","\n","# Use best model\n","model = grid_search.best_estimator_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Th6eT-_1Lu_e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Set: 0.9014670031612978\n"]}],"source":["# get the training error\n","f1_score(y_train, y_pred, average='macro')\n","# print for the train set f1 score is\n","print(\"Train Set: \" + str(f1_score(y_train, y_pred, average='macro')))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMLL1rU_n4BP"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.6313647451359965\n"]}],"source":["# get the prediction for the test set\n","test = pd.read_csv('./raw_data/balancedtest.csv')\n","X_test = np.array([preprocess_text(text) for text in test.iloc[:, 1]]) \n","y_test = test.iloc[:, 0].values - 1 # subtract 1 to make the labels 0-based\n","y_pred = model.predict(X_test)\n","\n","# get the f1 score\n","f1_score_test = f1_score(y_test, y_pred, average='macro')\n","print(f1_score_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["['./sklearn_models/logisticRegression 0.6313647451359965lemAndNoStopwordRemoval.pkl']"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["# save the sklearn model\n","import joblib\n","joblib.dump(model, './sklearn_models/logisticRegression ' + str(f1_score_test) + '_logisticRegression_GloVe.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO5ywtC5STPqHlkhNNBbfoT","name":"CS4248 Assignment 2.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
