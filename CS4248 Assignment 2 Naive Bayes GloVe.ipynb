{"cells":[{"cell_type":"markdown","metadata":{"id":"oyI5SA6HIoRH"},"source":["# CS4248 Project Group 23"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vS033yHSHyAx"},"outputs":[],"source":["# If you wish to run this on Google Colab, mount the Google drive by running this cell or click the `files` icon on the left navbar\n","# and click mount Google Drive (it takes some time to load)\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd \"/content/drive/My Drive/<The path to this notebook in your Google Drive>\"\n","!cd \"/content/drive/My Drive/<The path to this notebook in your Google Drive>\""]},{"cell_type":"code","execution_count":1,"metadata":{"id":"2nqSeqYQJ4vW"},"outputs":[],"source":["import pandas as pd\n","from sklearn.metrics import f1_score\n","import numpy as np"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Unzip raw_data.zip locally\n","import zipfile\n","with zipfile.ZipFile('raw_data.zip', 'r') as zip_ref:\n","    zip_ref.extractall()"]},{"cell_type":"markdown","metadata":{},"source":["Feature Engineering: Capture various features of the text (e.g. punctuation, stopwords, statement length). \n","Test out different tokenizers to capture their performance.\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Done. 400000  words loaded!\n"]}],"source":["import gensim.downloader as api\n","\n","def load_glove_model():\n","    glove_model = api.load('glove-wiki-gigaword-300')\n","    print(\"Done.\",len(glove_model),\" words loaded!\")\n","    return glove_model, glove_model.vector_size\n","\n","glove_model, glove_dim = load_glove_model()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import re\n","import string\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","\n","# todo parallelize this in future\n","\n","\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","\n","def preprocess_text(text):\n","    # Lowercase text\n","    text = text.lower()\n","    # Remove punctuation\n","    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n","    # tokenize\n","    words = word_tokenize(text)\n","    # Initialize lemmatizer\n","    lemmatizer = WordNetLemmatizer()\n","    # Apply lemmatization\n","    words = [lemmatizer.lemmatize(word) for word in words]\n","    # Get the GloVe vectors\n","    vectors = [glove_model[word] for word in words if word in glove_model]\n","    # If vectors is empty, return a vector of zeros\n","    if not vectors:\n","        print(\"No vectors found for the text: \", text)\n","        return np.zeros(glove_model.vector_size)\n","    return np.mean(vectors, axis=0)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"9PepPt_CL94x"},"outputs":[{"name":"stdout","output_type":"stream","text":["No vectors found for the text:  abfs daksl \n","No vectors found for the text:  abfs daksl \n"]},{"ename":"KeyError","evalue":"'Test'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[1;32mc:\\Users\\samyu\\OneDrive - National University of Singapore\\Y4 Sem 2\\CS4248\\Project\\CS4248-Project-Group23\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n","File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n","\u001b[1;31mKeyError\u001b[0m: 'Test'","\nThe above exception was the direct cause of the following exception:\n","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Fit the TF-IDF vectorizer on the preprocessed corpus\u001b[39;00m\n\u001b[0;32m      8\u001b[0m X_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([preprocess_text(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]]) \n\u001b[1;32m----> 9\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([preprocess_text(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtest\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m]) \n\u001b[0;32m     11\u001b[0m y_train \u001b[38;5;241m=\u001b[39m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVerdict\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# convert to binary- label 4 = trusted\u001b[39;00m\n\u001b[0;32m     12\u001b[0m y_test \u001b[38;5;241m=\u001b[39m test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVerdict\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# convert to binary- label 4 = trusted\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\samyu\\OneDrive - National University of Singapore\\Y4 Sem 2\\CS4248\\Project\\CS4248-Project-Group23\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n","File \u001b[1;32mc:\\Users\\samyu\\OneDrive - National University of Singapore\\Y4 Sem 2\\CS4248\\Project\\CS4248-Project-Group23\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n","\u001b[1;31mKeyError\u001b[0m: 'Test'"]}],"source":["train = pd.read_csv(\"./raw_data/fulltrain.csv\", names=['Verdict', 'Text'])\n","test = pd.read_csv(\"./raw_data/balancedtest.csv\", names=['Verdict', 'Text'])\n","\n","# Preprocess all documents in the corpus\n","preprocessed_corpus = [preprocess_text(text) for text in train['Text']]\n","\n","# Fit the TF-IDF vectorizer on the preprocessed corpus\n","X_train = np.array([preprocess_text(text) for text in train['Text']]) "]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["X_test = np.array([preprocess_text(text) for text in test['Text']]) \n","X_train = np.abs(X_train)\n","X_test = np.abs(X_test)\n","\n","y_train = train['Verdict'].apply(lambda x: 1 if x == 4 else 0) # convert to binary- label 4 = trusted\n","y_test = test['Verdict'].apply(lambda x: 1 if x == 4 else 0) # convert to binary- label 4 = trusted"]},{"cell_type":"markdown","metadata":{},"source":["Test out different kinds of models and find the most effective architectures.|"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["from sklearn.naive_bayes import GaussianNB, MultinomialNB\n","model = MultinomialNB()"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'alpha': 0.1, 'class_prior': [0.6, 0.4], 'fit_prior': True}\n"]}],"source":["from sklearn.model_selection import GridSearchCV\n","\n","# or perform hyperparameter tuning\n","# Create the hyperparameters grid\n","\n","param_grid = {\n","    'alpha': [0.1, 0.5, 1.0, 1.5, 2.0],\n","    'fit_prior': [True, False],  # Whether to learn class prior probabilities or not\n","    'class_prior': [None, [0.9, 0.1], [0.8, 0.2], [0.7, 0.3], [0.6, 0.4], [0.5, 0.5], [0.4, 0.6], [0.3, 0.7], [0.2, 0.8], [0.1, 0.9]]\n","}\n","\n","grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1)\n","\n","# Train\n","grid_search.fit(X_train, y_train)\n","\n","\n","hyperparams = str(grid_search.best_params_)\n","print (hyperparams)\n","\n","# Use best model\n","model = grid_search.best_estimator_"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training F1 score is: 0.6576363020656959\n","Test F1 score is: 0.6029798422436459\n"]}],"source":["model.fit(X_train, y_train)\n","y_pred_train = model.predict(X_train)\n","y_pred_test = model.predict(X_test)\n","print(\"Training F1 score is:\", f1_score(y_train, y_pred_train))\n","print(\"Test F1 score is:\", f1_score(y_test, y_pred_test))"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# load a previous model\n","import joblib\n","model = joblib.load(\"./sklearn_models/logisticRegression.pkl\")\n","y_pred = model.predict(x_train)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["['./sklearn_models/Naive Bayes GloVe 0.6029798422436459.pkl']"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# save the sklearn model\n","# best model now is {'alpha': 0.1, 'class_prior': [0.6, 0.4], 'fit_prior': True}\n","import joblib\n","joblib.dump(model, './sklearn_models/Naive Bayes GloVe '+ str(f1_score(y_test, y_pred_test)) + '.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO5ywtC5STPqHlkhNNBbfoT","name":"CS4248 Assignment 2.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
